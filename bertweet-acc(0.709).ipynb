{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/fairseq-and-fastbpe/sacrebleu-1.4.9-py3-none-any.whl\r\n",
      "Requirement already satisfied: typing in /opt/conda/lib/python3.7/site-packages (from sacrebleu==1.4.9) (3.7.4.1)\r\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu==1.4.9) (1.7.0)\r\n",
      "Installing collected packages: sacrebleu\r\n",
      "Successfully installed sacrebleu-1.4.9\r\n",
      "Processing /kaggle/input/fairseq-and-fastbpe/fairseq-0.9.0-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: cffi in /opt/conda/lib/python3.7/site-packages (from fairseq==0.9.0) (1.14.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from fairseq==0.9.0) (1.5.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from fairseq==0.9.0) (4.45.0)\r\n",
      "Requirement already satisfied: cython in /opt/conda/lib/python3.7/site-packages (from fairseq==0.9.0) (0.29.17)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fairseq==0.9.0) (1.18.1)\r\n",
      "Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.7/site-packages (from fairseq==0.9.0) (1.4.9)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from fairseq==0.9.0) (2020.4.4)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi->fairseq==0.9.0) (2.20)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->fairseq==0.9.0) (0.18.2)\r\n",
      "Requirement already satisfied: typing in /opt/conda/lib/python3.7/site-packages (from sacrebleu->fairseq==0.9.0) (3.7.4.1)\r\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu->fairseq==0.9.0) (1.7.0)\r\n",
      "Installing collected packages: fairseq\r\n",
      "Successfully installed fairseq-0.9.0\r\n",
      "Processing /kaggle/input/fairseq-and-fastbpe/fastBPE-0.1.0-cp37-cp37m-linux_x86_64.whl\r\n",
      "Installing collected packages: fastBPE\r\n",
      "Successfully installed fastBPE-0.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/fairseq-and-fastbpe/sacrebleu-1.4.9-py3-none-any.whl\n",
    "!pip install ../input/fairseq-and-fastbpe/fairseq-0.9.0-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install ../input/fairseq-and-fastbpe/fastBPE-0.1.0-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "import random\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tokenizers\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "warnings.filterwarnings('ignore')\n",
    "seed=18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from emoji import demojize\n",
    "import re\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def normalizeToken(token):\n",
    "    lowercased_token = token.lower()\n",
    "    if token.startswith(\"@\"):\n",
    "        return \"@USER\"\n",
    "    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
    "        return \"HTTPURL\"\n",
    "    elif len(token) == 1:\n",
    "        return demojize(token)\n",
    "    else:\n",
    "        if token == \"’\":\n",
    "            return \"'\"\n",
    "        elif token == \"…\":\n",
    "            return \"...\"\n",
    "        else:\n",
    "            return token\n",
    "\n",
    "def normalizeTweet(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
    "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "    normTweet = normTweet.replace(\"cannot \", \"can not \").replace(\"n't \", \" n't \").replace(\"n 't \", \" n't \").replace(\"ca n't\", \"can't\").replace(\"ai n't\", \"ain't\")\n",
    "    normTweet = normTweet.replace(\"'m \", \" 'm \").replace(\"'re \", \" 're \").replace(\"'s \", \" 's \").replace(\"'ll \", \" 'll \").replace(\"'d \", \" 'd \").replace(\"'ve \", \" 've \")\n",
    "    normTweet = normTweet.replace(\" p . m .\", \"  p.m.\") .replace(\" p . m \", \" p.m \").replace(\" a . m .\", \" a.m.\").replace(\" a . m \", \" a.m \")\n",
    "\n",
    "    normTweet = re.sub(r\",([0-9]{2,4}) , ([0-9]{2,4})\", r\",\\1,\\2\", normTweet)\n",
    "    normTweet = re.sub(r\"([0-9]{1,3}) / ([0-9]{2,4})\", r\"\\1/\\2\", normTweet)\n",
    "    normTweet = re.sub(r\"([0-9]{1,3})- ([0-9]{2,4})\", r\"\\1-\\2\", normTweet)\n",
    "    \n",
    "    return \" \".join(normTweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ` d have responded , if I were going'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizeTweet(' I`d have responded, if I were going')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path='../input/bertweet-dataset'\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    os.path.join(base_path,\"BERTweet_base_transformers/config.json\"), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data.encoders.fastbpe import fastBPE\n",
    "from fairseq.data import Dictionary\n",
    "args = argparse.Namespace(bpe_codes= os.path.join(base_path,\"BERTweet_base_transformers/bpe.codes\"))\n",
    "bpe = fastBPE(args)\n",
    "vocab = Dictionary()\n",
    "vocab.add_from_file(os.path.join(base_path,\"BERTweet_base_transformers/dict.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, bpe, vocab, max_len=96):\n",
    "        self.df = df\n",
    "        self.labeled = 'selected_text' in df\n",
    "        self.bpe = bpe\n",
    "        self.vocab = vocab\n",
    "        self.max_len=max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data={}\n",
    "        row=self.df.iloc[index]\n",
    "        #print(row.text)\n",
    "        #print(row.selected_text)\n",
    "        ids, masks, tweets_encoded = self.get_input_data(row)\n",
    "        data['ids'] = ids\n",
    "        data['masks'] = masks\n",
    "        data['tweets_encoded'] = tweets_encoded\n",
    "        data['tweet'] = row.text\n",
    "        data['sentiment']=row.sentiment\n",
    "        if self.labeled:\n",
    "            data['selected_tweet'] = row.selected_text\n",
    "            start_idx, end_idx = self.get_target_idx(row, tweets_encoded)\n",
    "            data['start_idx'] = start_idx\n",
    "            data['end_idx'] = end_idx\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def get_input_data(self, row):\n",
    "        normalized_tweets = normalizeTweet(row.text)\n",
    "        normalized_tweets = \" \" + \" \".join(normalized_tweets.split())\n",
    "        tweets_encoded = self.bpe.encode(normalized_tweets)\n",
    "        encoding_ids = self.vocab.encode_line(tweets_encoded, append_eos=False, add_if_not_exist=False).long().tolist()\n",
    "        sentiment_id = self.vocab.encode_line(self.bpe.encode(row.sentiment), append_eos=False, add_if_not_exist=False).long().tolist()\n",
    "        ids = [0]+sentiment_id+[2,2]+encoding_ids+[2]\n",
    "        \n",
    "        pad_len = self.max_len-len(ids)\n",
    "        if pad_len>0:\n",
    "            ids += [1] * pad_len\n",
    "        ids = torch.tensor(ids)\n",
    "        masks = torch.where(ids!=1, torch.tensor(1), torch.tensor(0))\n",
    "        \n",
    "        return ids, masks, tweets_encoded\n",
    "    \n",
    "    def get_target_idx(self, row, tweets_encoded):\n",
    "        normalized_selected_tweets = normalizeTweet(row.selected_text)\n",
    "        normalized_selected_tweets = ' '+' '.join(normalized_selected_tweets.split())\n",
    "        normalized_tweets = normalizeTweet(row.text)\n",
    "        normalized_tweets = \" \" + \" \".join(normalized_tweets.split())\n",
    "        #print(normalized_selected_tweets)\n",
    "        #print(normalized_tweets)\n",
    "        \n",
    "        len_st = len(normalized_selected_tweets) - 1\n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "        for ind in (i for i, e in enumerate(normalized_tweets) if e == normalized_selected_tweets[1]):\n",
    "            if \" \" + normalized_tweets[ind: ind+len_st] == normalized_selected_tweets:\n",
    "                idx0 = ind\n",
    "                idx1 = ind+len_st-1\n",
    "                break\n",
    "        if idx0==None and len(normalized_selected_tweets.split())>1:\n",
    "            normalized_selected_tweets_1=' '+' '.join(normalized_selected_tweets.split()[1:])\n",
    "            #print(normalized_selected_tweets_1)\n",
    "            len_st_1 = len(normalized_selected_tweets_1) - 1\n",
    "            for ind in (i for i, e in enumerate(normalized_tweets) if e == normalized_selected_tweets_1[1]):\n",
    "                if \" \" + normalized_tweets[ind: ind+len_st_1] == normalized_selected_tweets_1:\n",
    "                    idx0 = ind\n",
    "                    idx1 = ind+len_st_1-1\n",
    "                    break\n",
    "        if idx0==None and len(normalized_selected_tweets.split())>1:\n",
    "            normalized_selected_tweets_2=' '+' '.join(normalized_selected_tweets.split()[:-1])\n",
    "            #print(normalized_selected_tweets_2)\n",
    "            len_st_2 = len(normalized_selected_tweets_2) - 1\n",
    "            for ind in (i for i, e in enumerate(normalized_tweets) if e == normalized_selected_tweets_2[1]):\n",
    "                if \" \" + normalized_tweets[ind: ind+len_st_2] == normalized_selected_tweets_2:\n",
    "                    idx0 = ind\n",
    "                    idx1 = ind+len_st_2-1\n",
    "                    break\n",
    "        if idx0==None and len(normalized_selected_tweets.split())>1:\n",
    "            normalized_selected_tweets_3=' '+' '.join(normalized_selected_tweets_2.split()[:-1])\n",
    "            #print(normalized_selected_tweets_3)\n",
    "            len_st_3 = len(normalized_selected_tweets_3) - 1\n",
    "            for ind in (i for i, e in enumerate(normalized_tweets) if e == normalized_selected_tweets_3[1]):\n",
    "                if \" \" + normalized_tweets[ind: ind+len_st_3] == normalized_selected_tweets_3:\n",
    "                    idx0 = ind\n",
    "                    idx1 = ind+len_st_3-1\n",
    "                    break \n",
    "        sum_tot=-1\n",
    "        flag = 0\n",
    "        if idx0 != None and idx1 != None:\n",
    "            for i, token in enumerate(tweets_encoded.split()):\n",
    "                if '@@' not in token:\n",
    "                    sum_tot += len(token)+1\n",
    "                else:\n",
    "                    sum_tot += len(token)-2\n",
    "                if sum_tot>=idx0 and flag==0:\n",
    "                    start_idx = i\n",
    "                    flag = 1\n",
    "                if sum_tot>=idx1:\n",
    "                    end_idx = i\n",
    "                    break\n",
    "        if idx0==None or idx1==None:\n",
    "            start_idx=0\n",
    "            end_idx=0\n",
    "        return start_idx+4, end_idx+4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_loaders(df, train_idx, val_idx, batch_size=32):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(train_df, bpe, vocab), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        drop_last=False)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(val_df, bpe, vocab), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)\n",
    "\n",
    "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "    return dataloaders_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class BERTweetModel(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(BERTweetModel, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(os.path.join(base_path,\"BERTweet_base_transformers/model.bin\"),config=conf)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(conf.hidden_size*4,2)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.normal_(self.fc.bias,0)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        a, b, h = self.roberta(input_ids, attention_mask)\n",
    "        x = torch.cat([h[-1],h[-2],h[-3], h[-4]],dim=-1)\n",
    "        x = self.fc(self.dropout(x))\n",
    "        start_logits, end_logits = x.split(1, -1)\n",
    "        \n",
    "        return start_logits.squeeze(-1), end_logits.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    start_loss = ce_loss(start_logits, start_positions)\n",
    "    end_loss = ce_loss(end_logits, end_positions)    \n",
    "    total_loss = start_loss + end_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_text(tweets_encoded, start_idx, end_idx):\n",
    "    selected_text = \"\"\n",
    "    for i, token in enumerate(tweets_encoded.split()[start_idx-4:end_idx-3]):\n",
    "            token=' '+token\n",
    "            selected_text+=token\n",
    "    selected_text=re.sub('@@ ', '', selected_text)\n",
    "    selected_text=re.sub('@@', '', selected_text)\n",
    "    return selected_text\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def compute_jaccard_score(tweets_encoded, start_idx, end_idx, start_logits, end_logits):\n",
    "    start_pred = np.argmax(start_logits)\n",
    "    end_pred = np.argmax(end_logits)\n",
    "    #print('labels, outputs',start_idx, end_idx, start_pred, end_pred)    \n",
    "    length = len(tweets_encoded.split())\n",
    "    if start_pred<4:\n",
    "        start_pred=4\n",
    "    if end_pred>3+length:\n",
    "        end_pred=3+length\n",
    "    if start_pred > end_pred:\n",
    "        start_pred=4\n",
    "        end_pred=3+length\n",
    "        pred = get_selected_text(tweets_encoded, start_pred, end_pred).strip()\n",
    "    else:\n",
    "        pred = get_selected_text(tweets_encoded, start_pred, end_pred).strip()    \n",
    "    true = get_selected_text(tweets_encoded, start_idx, end_idx).strip()\n",
    "    #print(true)\n",
    "    #print(pred)\n",
    "    \n",
    "    return jaccard(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    loss_check=1000\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            epoch_jaccard = 0.0\n",
    "            count=0\n",
    "            for data in (dataloaders_dict[phase]):\n",
    "                if count%100==0:\n",
    "                    print(count)\n",
    "                count+=1\n",
    "                ids = data['ids']\n",
    "                masks = data['masks']\n",
    "                tweets_encoded = data['tweets_encoded']\n",
    "                \n",
    "                selected_tweet = data['selected_tweet']\n",
    "                start_idx = data['start_idx']\n",
    "                end_idx = data['end_idx']\n",
    "                #print(tweets_encoded[0])\n",
    "                #print(tweets_encoded[1])\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                  ids=ids.cuda()\n",
    "                  masks=masks.cuda()\n",
    "                  start_idx=start_idx.cuda()\n",
    "                  end_idx=end_idx.cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    start_logits, end_logits = model(ids, masks)\n",
    "\n",
    "                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item() * len(ids)\n",
    "                    \n",
    "                    start_idx = start_idx.cpu().detach().numpy()\n",
    "                    end_idx = end_idx.cpu().detach().numpy()\n",
    "                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n",
    "                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n",
    "                    \n",
    "                    for i in range(len(ids)): \n",
    "                        #print(selected_tweet[i])                       \n",
    "                        jaccard_score = compute_jaccard_score(\n",
    "                            tweets_encoded[i],\n",
    "                            start_idx[i],\n",
    "                            end_idx[i],\n",
    "                            start_logits[i], \n",
    "                            end_logits[i])\n",
    "                        epoch_jaccard += jaccard_score\n",
    "                    \n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n",
    "            \n",
    "            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n",
    "                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n",
    "        if epoch_loss<loss_check:\n",
    "            loss_check=epoch_loss\n",
    "            print(\"Saving model\")\n",
    "            torch.save(model.state_dict(), filename)\n",
    "        elif epoch>1:\n",
    "            print('Training stopping')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "skf = StratifiedKFold(n_splits=8, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    train_df = pd.read_csv('tweet-sentiment-extraction/train.csv').dropna().reset_index(drop=True)\n",
    "    train_df['text'] = train_df['text'].astype(str)\n",
    "    train_df['selected_text'] = train_df['selected_text'].astype(str)\n",
    "\n",
    "    (train_idx, val_idx) = list(skf.split(train_df, train_df.sentiment))[fold]\n",
    "    print(f'Fold: {fold}')\n",
    "    model = BERTweetModel(conf=config)\n",
    "    torch.save(model.state_dict(), f'drive/My Drive/Kaggle/check.pth')\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-5, betas=(0.9, 0.999))\n",
    "    criterion = loss_fn    \n",
    "    dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n",
    "    print('starting training')\n",
    "    train_model(\n",
    "        model, \n",
    "        dataloaders_dict,\n",
    "        criterion, \n",
    "        optimizer, \n",
    "        num_epochs,\n",
    "        f'drive/My Drive/Kaggle/roberta_fold{fold}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loader(df, batch_size=32):\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(df, bpe, vocab), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path='../input/bertweet-dataset'\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    os.path.join(base_path,\"BERTweet_base_transformers/config.json\"), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(pred, tweet):\n",
    "    pred_wo_spaces=''.join(pred.split())\n",
    "    length = len(pred_wo_spaces)\n",
    "    flag=0\n",
    "    if tweet[-1]=='@':\n",
    "        return tweet\n",
    "    else:\n",
    "        for index, value in enumerate(tweet):\n",
    "            count=0\n",
    "            letter=pred_wo_spaces[count]\n",
    "            if value==letter:\n",
    "                start_idx=index\n",
    "                end_idx=index\n",
    "                count+=1\n",
    "                end_idx+=1\n",
    "                while True:\n",
    "                    if tweet[end_idx]==' ':\n",
    "                        end_idx+=1\n",
    "                    elif tweet[end_idx]=='!' and pred_wo_spaces[count]!='!':\n",
    "                        end_idx+=1\n",
    "                    elif tweet[end_idx]=='.' and pred_wo_spaces[count]!='.':\n",
    "                        end_idx+=1\n",
    "                    elif tweet[end_idx]=='*' and pred_wo_spaces[count]!='*':\n",
    "                        end_idx+=1\n",
    "                    elif tweet[end_idx]=='-' and pred_wo_spaces[count]!='-':\n",
    "                        end_idx+=1\n",
    "                    elif tweet[end_idx]=='?' and pred_wo_spaces[count]!='?':\n",
    "                        end_idx+=1\n",
    "                    elif tweet[end_idx]==pred_wo_spaces[count]:\n",
    "                        end_idx+=1\n",
    "                        count+=1\n",
    "                    else:\n",
    "                        break\n",
    "                    if count==length:\n",
    "                        flag=1\n",
    "                        break\n",
    "                if flag==1:\n",
    "                    break\n",
    "        if flag==1:\n",
    "            #print(tweet[start_idx:end_idx])\n",
    "            while start_idx>0:\n",
    "                if tweet[start_idx-1]==' ':\n",
    "                    break\n",
    "                else:\n",
    "                    print('start_idx_changed')\n",
    "                    start_idx=start_idx-1\n",
    "            while end_idx<len(tweet)-1:\n",
    "                if tweet[end_idx]==' ':\n",
    "                    break\n",
    "                else:\n",
    "                    print('end_idx_changed')\n",
    "                    end_idx=end_idx+1\n",
    "            #print(tweet[start_idx:end_idx])\n",
    "            return tweet[start_idx:end_idx]\n",
    "        elif 'HTTPURL' in pred.split():\n",
    "            return tweet\n",
    "        elif '@USER' in pred.split():\n",
    "            return tweet\n",
    "        else:\n",
    "            print('No match found')\n",
    "            print(pred)\n",
    "            print(tweet)\n",
    "            return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "No match found\n",
      "Guys will go see Constance logically\n",
      "Guys will go see Constance \t logically\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "end_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n",
      "start_idx_changed\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "test_df['text'] = test_df['text'].astype(str)\n",
    "#test_df['selected_text'] = test_df['selected_text'].astype(str)\n",
    "\n",
    "test_loader = get_test_loader(test_df)\n",
    "predictions = []\n",
    "models = []\n",
    "for fold in range(skf.n_splits):\n",
    "    model = BERTweetModel(conf=config)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.load_state_dict(torch.load(f'../input/mosh1-data-orig/roberta_fold{fold}.pth',map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "count=0\n",
    "count_real=0\n",
    "jaccard_total=0\n",
    "for data in test_loader:\n",
    "    #print(count)\n",
    "    ids = data['ids']\n",
    "    masks = data['masks']\n",
    "    tweets_encoded = data['tweets_encoded']\n",
    "    tweet = data['tweet']\n",
    "    sentiment = data['sentiment']\n",
    "    \n",
    "    #selected_tweet = data['selected_tweet']\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        ids=ids.cuda()\n",
    "        masks=masks.cuda()\n",
    "\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for model in models:\n",
    "        with torch.no_grad():\n",
    "            output = model(ids, masks)\n",
    "            start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
    "            end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
    "    \n",
    "    start_logits = np.mean(start_logits, axis=0)\n",
    "    end_logits = np.mean(end_logits, axis=0)\n",
    "    for i in range(len(ids)):  \n",
    "        count_real+=1\n",
    "        #print('Coount', count_real)\n",
    "        start_pred = np.argmax(start_logits[i])\n",
    "        end_pred = np.argmax(end_logits[i])\n",
    "        #print(tweet[i].strip())\n",
    "        #print(tweets_encoded[i])\n",
    "        length = len(tweets_encoded[i].split())\n",
    "        if start_pred<4:\n",
    "            start_pred=4\n",
    "        if end_pred>3+length:\n",
    "            end_pred=3+length\n",
    "        if start_pred > end_pred:\n",
    "            start_pred=4\n",
    "            end_pred=3+length\n",
    "            pred = get_selected_text(tweets_encoded[i], start_pred, end_pred).strip()\n",
    "        else:\n",
    "            pred = get_selected_text(tweets_encoded[i], start_pred, end_pred).strip()    \n",
    "        #print(pred,'\\n')\n",
    "        try:\n",
    "            pred=postprocessing(pred, tweet[i].strip())\n",
    "        except:\n",
    "            print('Error')\n",
    "            print(tweet[i].strip())\n",
    "            print(pred)\n",
    "            pred=tweet[i]\n",
    "        \n",
    "        #this if loop is only for testing purposes on train data\n",
    "        #if pred.strip()!=selected_tweet[i].strip():\n",
    "            #count+=1\n",
    "            #print(count)\n",
    "            #print(sentiment[i]) \n",
    "            #print(tweet[i].strip())\n",
    "            #print(selected_tweet[i].strip())\n",
    "            #print(pred.strip())\n",
    "            #jaccard_total+=jaccard(selected_tweet[i].strip(),pred.strip())\n",
    "        #this above if loop only for testing purposes\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        \n",
    "#print('Count', count)\n",
    "#print('Average jaccard', jaccard_total/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>exciting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>I like it!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                      selected_text\n",
       "0  f87dea47db                            Last session of the day\n",
       "1  96d74cb729                                           exciting\n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...\n",
       "3  01082688c6                                        happy bday!\n",
       "4  33987a8ee5                                        I like it!!"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "sub_df['selected_text'] = predictions\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "sub_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
